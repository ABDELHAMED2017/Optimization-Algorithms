# Optimization-Algorithms

## First Order Optimization Algorithms

```
1. Batch Gradient Descent
2. Stochastic Gradient Descent
3. Mini Batch Stochastic Gradient Descent
4. Momentum
5. Nesterov Accelarated Gradient
6. AdaGrad
7. RMSProp
8. RMSProp with Nesterov Momentum
9. Adam
10. ADADelta
11. Linear Conjugate Gradient Method
12. Non Linear Conjugate Gradient Method
13. AdaMax
14. Nadam
15. AMSGrad
16. kSGD
```

## Second Order Optimization Algorithms

```
1. Newton's Method
2. Quasi- Newton Method
3. Gauss-Newton Method
4. Broyden-Fletcher-Goldfarb-Shanno (BFGS)
5. Limited Memory BFGS
```
